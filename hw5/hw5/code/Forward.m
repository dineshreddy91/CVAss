function [output, act_a, act_h] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer pre activations in 'act_h', and the hidden layer post
% activations in 'act_a'.

act_a{1} = X * W{1} + b{1}';
act_h{1} = 1./(1+exp(-act_a{1}));
act_a{2} = act_h{1} * W{2} + b{2}';
A = exp(act_a{2});
act_h{2} = bsxfun(@rdivide,A,sum(A')');
output = act_h{2};
end
