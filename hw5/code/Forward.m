function [output, act_h, act_a] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer pre activations in 'act_a', and the hidden layer post
% activations in 'act_h'.

l = length(W);
act_a = cell(1,l-1);
act_h = cell(1,l-1);
x = X;
%hidden layers are l-1
for i=1:l-1
    act_a{i} = W{i}*x + b{i};
    % apply signmoid layer to hidden variables
    act_h{i} = 1./(1+exp(-act_a{i}));
    x = act_h{i};
end
%output with softmax
y = exp(W{l}*act_h{l-1} + b{l});
output = y./sum(y);
